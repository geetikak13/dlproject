# Anomaly Detection in Network Traffic using a Transformer Autoencoder

**Project for Course 612, Group: Bumblebee**

This project implements a Transformer-based autoencoder to detect Distributed Denial of Service (DDoS) attacks in network traffic. The model is trained exclusively on benign traffic and identifies anomalies by flagging sequences with high reconstruction errors. This approach is designed to detect not only known attack patterns but also novel, zero-day threats.

## Project Structure

```
ddos_anomaly_detection/
│
├── data/
│   ├── raw/          # Raw CIC-DDoS2019 dataset files go here
│   └── processed/    # Processed data files generated by the notebook
│
├── notebooks/
│   ├── 01_data_exploration.ipynb
│   ├── 02_model_prototyping.ipynb
│   └── 03_results_visualization.ipynb
│
├── src/
│   ├── config.py
│   ├── data_loader.py
│   ├── model.py
│   ├── train.py
│   ├── evaluate.py
│   ├── explainability.py
│   └── utils.py
│
├── saved_models/     # Saved PyTorch model (.pth) files
│
├── results/
│   ├── performance_metrics.json
│   └── attention_maps/
│
├── requirements.txt  # Project dependencies
└── README.md         # This file
```

## Setup and Installation

### Prerequisites

* Python 3.8+
* Pip package manager

### 1. Clone the Repository

```bash
git clone <your-repository-url>
cd ddos_anomaly_detection
```

### 2. Create a Virtual Environment (Recommended)

Using a virtual environment prevents package conflicts with other projects.

```bash
# For macOS/Linux
python3 -m venv venv
source venv/bin/activate

# For Windows
python -m venv venv
.\venv\Scripts\activate
```

### 3. Install Dependencies

Install all the required packages from the `requirements.txt` file.

```bash
pip install -r requirements.txt
```

### 4. Download the Dataset

1.  Download the **CIC-DDoS2019** dataset from the [Canadian Institute for Cybersecurity](https://www.unb.ca/cic/datasets/ddos-2019.html).
2.  Extract the CSV files from the downloaded archive.
3.  Place all the CSV files inside the `data/raw/` directory.

## How to Run the Project

Follow these steps in order to ensure the project runs correctly.

### Step 1: Preprocess the Data

The first step is to run the data exploration notebook. This will load the raw data, clean it, and create the processed files required for training and testing.

1.  Start Jupyter Notebook:
    ```bash
    jupyter notebook
    ```
2.  Open and run all cells in `notebooks/01_data_exploration.ipynb`.

This will generate `benign_traffic_processed.csv` and `test_traffic_processed.csv` in the `data/processed/` directory.

### Step 2: Train the Model

Run the training script from the project's root directory. This script trains the Transformer autoencoder on the benign traffic and saves the trained model to the `saved_models/` directory.

```bash
python src/train.py
```

### Step 3: Evaluate the Model

After training is complete, run the evaluation script. This will test the model's ability to distinguish between benign and malicious traffic.

```bash
python src/evaluate.py
```

This script will output:
* Performance metrics (Accuracy, Precision, Recall, F1-Score) to the console.
* A `performance_metrics.json` file in the `results/` directory.
* A `confusion_matrix.png` visualization in the `results/` directory.

### Step 4: Interpret an Anomaly (Explainable AI)

To understand *why* the model flags a sequence as an anomaly, run the explainability script.

```bash
python src/explainability.py
```

This script will find an anomalous sequence from the test set, pass it through the model to get attention weights, and save heatmap visualizations to the `results/attention_maps/` directory.

### Using the Notebooks

The other notebooks can be used for interactive development and analysis:
* `notebooks/02_model_prototyping.ipynb`: A sandbox for interactively building and testing the model with a smaller number of epochs.
* `notebooks/03_results_visualization.ipynb`: A dashboard to load and visualize the final results (metrics, confusion matrix, attention maps) generated by the Python scripts.

## Citation

This project adapts principles from time-series anomaly detection using Transformers. A key reference is:

> Xu, J., et al. (2022). "Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy." *ICLR 2022*.
